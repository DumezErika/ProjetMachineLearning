\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bbold}

\usepackage{graphicx}
\usepackage{systeme}
\usepackage{stmaryrd}

\theoremstyle{definition}
\newtheorem{definition}{Définition}
\theoremstyle{definition}
\newtheorem{theorem}{Théorème}

\author{Della Bona Sarah, Dumez Erika}
\title{Introduction to neural ODE}
\begin{document}
\maketitle
 
\section{Ordinary Differential Equations}

\subsection{A reminder on ODE}

An ODE is a function that describes the changes of a function, $u$, through time. In this setting, time is a continuous variable and we don't know the real function, only its derivative.

\begin{definition}
Let $f: \Omega \subseteq \mathbb{R} \times \mathbb{R}^N \rightarrow \mathbb{R}^N$. 

A \textit{first order ODE} takes of the form:
\[
\partial_t u(t) = f(t,u(t)) \textbf{   (1)}
\]

\begin{itemize}
\item A \textit{solution} for (1) is a function $u : I \rightarrow \mathbb{R}^N$ where $I$ is an interval of $\mathbb{R}$ such that:
	\begin{itemize}
	\item u is derivable on I,
	\item $\forall t \in I, f(t, u(t)) \in \Omega$,
	\item $\forall t \in I, \partial_t u(t) = f(t, u(t))$
	\end{itemize}
\item An \textit{initial condition} (IC) is a condition of the type:
\[
u(t_0) = u_0
\]
where $(t_0, u_0) \in \Omega$ is fixed.
\end{itemize}
\item A \textit{Cauchy problem} is an ODE with IC
\[
\left \{
\begin{array}{rcl}
\partial_t u(t) & = & f(t, u(t)) \\
u(t_0) & = & u_0
\end{array}
\right.
\]
\end{definition}

\begin{definition}
A \textit{k-order ODE} is of the form:
\[
\partial^k_t v(t) = g(t, v(t), ... , \partial^{k-1}v(t))
\]
where 
   \begin{eqnarray}
   \nonumber
   v & : & I \rightarrow \mathbb{R}^N \\ 
   \nonumber
   g & : & \Theta \subseteq \mathbb{R} \times \mathbb{R}^N \times ... \times \mathbb{R}^N \rightarrow \mathbb{R}^N
   \end{eqnarray}
\end{definition}

It is not always possible to explicitly find a solution to a Cauchy problem, but we can compute a finite number of points $u_i \in \mathbb{R}^N$ which are close to the real solution. 

More precisely, let $T \in \mathbb{R}\backslash\{0\}$ such that the solution $u$ exists on $[t_0, t_0 + T]$ and let $n \in \mathbb{N}^{\geqslant 2}$. We are then looking for $(u_i)^n_{i=0}$ s.t. 
\[
u_i \approx u(t_i) \ \text{ where } t_0 < ... < t_n \in [t_0, t_0 + T]
\]

Let $h_i := t_{i+1} - t_i$, it is called the \textit{step}. To compute those $u_i$, we use \textit{1-step methods} such as Euler's method.

\subsection{Euler's method}

Euler's method is similar to a Taylor development, the idea is to compute $u(t_{i+1})$ using the following formula:\footnote{We consider that $ \forall i\ \{0,...,n\}, h_i = h$.}
\[
u(t_{i+1}) \approx u(t_i) + h . \partial u(t_i)
\]
where 
\[
\partial u(t_i) = f(t_i, u(t_i)).
\]

\section{Neural networks}

In a typical machine learning problem, you are given some input $x$ and you want to predict an output $y$. A \textit{neural network} can be used to solve such a problem. It consists of a series of layers. There are three types of layers :

\begin{itemize}
\item The \textit{input} layer
\item The \textit{output} layer
\item The \textit{hidden} layers
\end{itemize}

Each layer consist of a certain number of neurons. We give an input to the neurons of a layer, they do some calculus (non-linear activation function for example) and they give an output. Therefore, layers can be seen has matrix operations. 

The neurons of a layer are connected to the neurons of the next layer. Thanks to these connections, the output given by a neuron can be transmitted over the neural network. We begin by giving an input to the input layer, which transmits information to the first hidden layer (There isn't always an hidden layer in a neural network. In this case, the input layer is directly connected to the output layer). In turn, it transmit information to the next hidden layer and so on, until the output layer which give us the final output, the prediction. 

With a loss function, we can then determined the accuracy of the neural network and make it more accurate by changing the parameters. We are trying to find the optimal parameters which minimize this loss function. In order to do that, we use gradient descent.

We can see a neural network as a composite of functions, a function for each layer. That is because each layer uses an activation function on its input. Each layer introduces a little bit of error that propagate through the network.

\begin{center}
\includegraphics[scale=0.55]{nn.png}
\end{center}  

\subsection{Example} \label{2.1}

Let's consider a neural network, with one hidden layer, that takes a 2-dimensional input $x = (x_1, x_2)$, and gives a 2-dimensional output $y = (y_1,y_2)$. We can represent this network with the following equations:

   \begin{eqnarray}
   \nonumber
   z_i & = & \sum_{j=1}^2 w_{ij}^{(1)}x_j + b_i^{(1)} \text{ pour } i = 1,2 \\ 
   \nonumber
   h_i & = & \sigma (z_i) \text{ pour } i=1,2 \\
   \nonumber
   y_k & = & \sum_{i=1}^2 w_{ki}^{(2)}h_i + b_k^{(2)} \text{ pour } k = 1,2 \\
   \nonumber
   \mathcal{L} & = & \frac{1}{2} \sum_{k = 1}^2 (y_k - t_k)^2
   \end{eqnarray}
   
where $w^{(1)}$, $w^{(2)}$, $b^{(1)}$ and $b^{(2)}$ are parameters of the network, and $t = (t_1, t_2)$ is the value we want to approximate (the "real" output for $x$). 

\subsection{Back propagation}

\section{Residual neural network} \label{rnn}

A \textit{residual neural network} is simply a regular network except that they have more connections. Indeed, not only do we feed the output of the previous layer to the next, but also the input of that layer. Thanks to that, residual networks have a better accuracy. 

In these networks, the $k+1$th layer has the formula:
\[
x_{k+1} = x_k + F(x_k)
\]
where $F$ is the function of the $k$th layer and its activation. This simple formula is a special case of the formula:
\[
x_{k+1} = x_k + h.F(x_k),
\]
which is the formula for the Euler method for solving ordinary differential equations (ODEs) when $h = 1$. It is with this observation that we can later introduce neural ODE.

\section{Implicit Layers}

There is two different ways to define a layer : \textit{explicitly} or \textit{implicitly}. When we define a layer explicitly, we specify the exact sequence of operations to do from the input to the output layer like in the example of the section \ref{2.1}. 

However, when we add some functionality to the layers, it can become complex to define them explicitly. Instead, we should define them implicitly. In this case, we specify the condition we want the layer's output to satisfy. 

In other words, lets assume that we have an input space $\mathcal{X}$ and an output space $\mathcal{Y}$. Then we define the explicit layers by a function $f : \mathcal{X} \rightarrow \mathcal{Y}$ such as $y = f(x)$ for some $x \in \mathcal{X}$. But we define an implicit layer by giving the condition that the function $g: \mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R}^n$ should satisfy (for example we can search for a $y$ such that $g(x,y) = 0$).

A special case is a layer that uses an ODE solver. For each layer $t$ we define the output $y(t)$ as the solution of the ODE:
$$\partial_t y(t) = f(t, y(t)), \qquad y(0) = y_0.$$

%TODO À retravailler !!!

\subsection{Implicit function theorem}
Let $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ be a function.

We denote the derivative of $f$ evaluated at a point $x \in \mathbb{R}^n$ as :

\[ \partial f : \mathbb{R}^n \rightarrow \mathbb{R}^m \]

We can write the first-order Taylor's development for $f$ at $x$ as :

\[ f(x + a) = f(x) + a . \partial f(x) + O(\| a\|^2) \]


where $a \in \mathbb{R}^n$ is a vector.

We also use the following notation :

\[ \partial_0 f(x,y) = \frac{\partial f(x,y)}{\partial x}  \]
\[ \partial_1 f(x,y) = \frac{\partial f(x,y)}{\partial y}  \]

\begin{theorem}{\textbf{The implicit function theorem}}

Let $f: \mathbb{R}^p \times \mathbb{R}^n \rightarrow \mathbb{R}^n$ be a function and $a_0 \in \mathbb{R}^p , z_0 \in \mathbb{R}^n$ two vectors such that :

\begin{enumerate}
\item $f(a_0,z_0) = 0$;
\item $f$ is continuously differentiable with a non-singular Jacobian, i.e. its determinant is non zero, $\partial_1 f(a_0,z_0) \in \mathbb{R}^{n \times n}$.
\end{enumerate}
Then there exist open sets $S_{a_0} \subset \mathbb{R}^p$ and $S_{z_0} \subset \mathbb{R}^n$ containing $a_0$ and $z_0$, respectively, and a unique continuous function $z*:S_{a_0} \rightarrow S_{z_0}$ such that:
\begin{itemize}
\item $z_0=z^*(a_0)$,
\item $ \forall a \in S_{a_0}, f(a,z^*(a))=0$,
\item $z^*$ is differentiable on $S_{a_0}$.
\end{itemize}
\end{theorem}


\section{Neural ODE}

\subsection{Définition}

%(Here, let's consider a neural network as a continuous function with no grouping, no blocks.   We normally have to decide how many layers we want in our neural network, and then we can build the network. With this idea, instead of specifying the number of layers, we need to specify the desired accuracy of the function, and it will learn how to train itself within that margin of error. )

In a residual neural network, the output for input $x$ is a function $F(x, \Theta)$ where $\Theta$ represents the parameters of each layer. 

But we want to extract all these individual layers and only have one "shared" layer.

%TODO textbf{insérer des dessins!!!}

Any output of the layer of a residual network can be computed in the ODE network with the function:
$$F(z_t, t, \Theta)$$
with $t$ being the number of that layer of the network minus one.

In the figure ?, the output of the $k$th layer is:
$$z_k = f(z_{k-1}, k-1) + z_{k-1} = F(z_{k-1}, k-1, \Theta).$$
We can then view $z$ as a function of $t$. For example,
$$z(1) = f(x, 0) + x.$$

We can also write $F$ as a continuous function of $t$, so that $F$ is no longer a function of $z$. However, we need to give it the initial value of $z$, which is $z(0) = x$ (the input).

Let's now consider that the value given by $F(z(t), t, \Theta)$ is the derivative of $z(t)$. Thus, instead of doing $z(t) = F(z(t), t, \Theta)$, we put the model on the derivative, $z'(x) = F(z(t), t, \Theta)$.  We obtain the following ODE:
$$ \partial_t z(t) = F(z(t), t, \Theta) $$
where the initial condition for "time" $0$ is $z(0) = x$. 

To get the output we will have to solve this ODE.

The final condition at a certain "time" (the number of layers) will be the desired output of the neural network.


\subsection{Forward pass}

With the above, we have that the output of the residual neural network is given by
$$ F(z(n), n, \Theta) $$
where $n$ is the number of layers. 
In the ODE neural network, instead of having multiple individual layers, the entire network is one continuous block of computation. This means that we do not need to specify the number of layers beforehand.

We observe that in this case, the layer is defined implicitly by the ODE with initial condition:
$$ \partial_t z(t) = F(z(t), t, \Theta). $$

But how do we find the solution to this ODE, i.e. the output? We can simply use an ODE Solver, like Euler method or Runge-Kutta for example. In the case of the Euler method, the result is equivalent to a residual neural network, as we saw in section \ref{rnn}.

%TODO ici du code !!!


\subsection{Backward pass: the Adjoint method}
Now that we know how to calculate the output from the input and the parameter $\theta$, we need a method to find the optimal $\theta$ that minimize the loss function.

In regular neural networks, we usually use the gradient descent. However in our case, it is more difficult because we used an ODE solver in the forward pass which is some sort of black box. This is why we are introducing the \textit{adjoint method}. This method computes the gradient by solving a second ODE backwards and is applicable to all ODE solvers.

Let $L$ be a loss function. Then the error for an input $z(t_0)$ is given by :
$$
L(z(t_1)) = L \big( z(t_0) + \int_{t_0}^{t_1} f(z(t),t,\theta) dt \big) = L(ODESolve(z(t_0),f,t_0,t_1,\theta))
$$
% car f(..) = d(z) et z(1) = z(0) + z(1) - z(0)

To minimize the loss function $L$, we need gradients with respect to $\theta$. To achieve that, we first need to determine how the gradient of the loss depends on the hidden state $z(t)$ for each $t$, which is $\frac{\partial L}{\partial z(t)}$. This quantity is called the \textit{adjoint} and is noted $a(t)$. We would like to determine its dynamics, so we need to compute its derivative with respect to $t$. \footnote{Here we see the vectors as row vectors.}

With a continuous hidden state, we can write the transformation after an $\varepsilon$ change in time as :
$$
z(t+\varepsilon) = \int^{t+\varepsilon}_{t} f(z(t),t,\theta) dt + z(t)
$$
Let $ G : \varepsilon \mapsto \int^{t+\varepsilon}_{t} f(z(t),t,\theta) dt + z(t)$.

We can apply the Chain rule and we have :
$$
\frac{\partial L}{\partial z(t)} = \frac{\partial L}{\partial z(t+\varepsilon)} \frac{\partial z(t+\varepsilon)}{\partial z(t)}
$$
In other words :
$$
a(t) = a(t+\varepsilon)\frac{\partial G(\varepsilon)}{\partial z(t)} \text{   (1)}
$$

We have :
\begin{align*}
\frac{\partial a(t)}{\partial t} &= \lim_{\varepsilon \rightarrow 0^+} \frac{a(t+\varepsilon) - a(t)}{\varepsilon} \text{ by définition.}\\
&= \lim_{\varepsilon \rightarrow 0^+} \frac{a(t+\varepsilon) - a(t+\varepsilon)\frac{\partial G(\varepsilon)}{\partial z(t)}}{\varepsilon} \text{ by (1).}\\
&= \lim_{\varepsilon \rightarrow 0^+} \frac{a(t+\varepsilon) - a(t+\varepsilon)\frac{\partial z(t) + \varepsilon f(z(t),t,\theta) + \mathcal{O}(\varepsilon^2)}{\partial z(t)}}{\varepsilon} \text{ by Taylor's development of G in 0.} \\
&= \lim_{\varepsilon \rightarrow 0^+} \frac{a(t+\varepsilon) - a(t+\varepsilon)(\mathbb{1} + \varepsilon \frac{\partial f(z(t),t,\theta)} {\partial z(t)}+ \mathcal{O}(\varepsilon^2))}{\varepsilon}\\
&= \lim_{\varepsilon \rightarrow 0^+} \frac{-\varepsilon a(t+\varepsilon) \frac{\partial f(z(t),t,\theta)} {\partial z(t)}+ \mathcal{O}(\varepsilon^2)}{\varepsilon}\\
&= \lim_{\varepsilon \rightarrow 0^+} - a(t+\varepsilon) \frac{\partial f(z(t),t,\theta)} {\partial z(t)}+ \mathcal{O}(\varepsilon)\\
&= -a(t)\frac{\partial f(z(t),t,\theta)} {\partial z(t)}
\end{align*}
%TODO regarder d'où sort la transposée
 
We need to solve an ODE for the adjoint backwards in time. The constraint on the last time point, which is simply the gradient of the loss with respect to this point, has to be specified:
$$
a(t_N) = \frac{\partial L}{\partial z(t_N)}
$$
Then, the gradients with respect to the hidden state can be calculated at any time, including the initial value:
\begin{align*}
a(t_0) &= a(t_N) + \int^{t_0}_{t_N} \frac{\partial a(t)}{\partial t} dt\\
&= a(t_N) - \int^{t_0}_{t_N} a(t)^T \frac{\partial f(z(t),t,\theta)} {\partial z(t)} dt
\end{align*}
Hence, the dynamics of the adjoint are given by another ODE:
$$
\frac{\partial a(t)}{\partial t} = -a(t)^T \frac{\partial f(z(t),t,\theta)} {\partial z(t)} \text{ (2)}
$$

If we want to compute the gradients with respect to the parameters $\theta$, we have to evaluate a third integral, which depends on both $z(t)$ and $a(t)$:
$$
\frac{\partial L}{\partial \theta} = - \int^{t_0}_{t_N} a(t)^T \frac{\partial f(z(t),t,\theta)} {\partial \theta} dt
$$

To avoid computing each ODE on its own, we can do all of them at the same time. To do that we can generalize the ODE to:
$$
\frac{\partial}{\partial t} \begin{bmatrix}
							z \\ \theta \\ t
							\end{bmatrix} (t) 
= f_{aug}([z,\theta ,t]) := \begin{bmatrix}
							f([z,\theta ,t]) \\ 0 \\ 1
							\end{bmatrix},
$$

$$
a_{aug} := \begin{bmatrix}
			a \\ a_{\theta} \\ a_t
			\end{bmatrix}, \ 
a(t) = \frac{\partial L}{\partial z(t)}, \ 
a_\theta (t) = \frac{\partial L}{\partial \theta (t)}, \ 
a_t(t) := \frac{\partial L}{\partial t(t)}.
$$

The jacobian of $f$ has the form:

$$
\frac{\partial f_{aug}}{\partial [z,\theta,t]} = \begin{bmatrix}
\frac{\partial f}{\partial z} & \frac{\partial f}{\partial \theta} & \frac{\partial f}{\partial t} \\
\textbf{0} & \textbf{0} & \textbf{0} \\
\textbf{0} & \textbf{0} & \textbf{0}
\end{bmatrix}(t)
$$
where each \textbf{0} is a matrix of zeros with the corresponding dimensions.

We can use $a_{aug}$ in (2) and we get:
\begin{align*}
\frac{\partial a_{aug}(t)}{\partial t} 
&= - [a(t) \ a_\theta (t) \ a_t (t)]\frac{\partial f_{aug}}{\partial [ z,\theta , t]}(t) \\
&= -\Big[a\frac{\partial f}{\partial z} \ a\frac{\partial f}{\partial \theta} \ a\frac{\partial  f}{\partial t}\Big] (t)
\end{align*}

%(52) la dérivée de L selon t_N c'est la chain rule avec z(t_N), la deuxième c'est avec l'intégrale 



% The vector-Jacobian products $a(t)^T \frac{\partial f(z(t),t,\theta)} {\partial z(t)}$ and $a(t)^T \frac{\partial f(z(t),t,\theta)} {\partial \theta}$ can be efficiently evaluated by automatic differentiation, at a time cost similar to that of evaluating $f$. All integrals for solving $z, a$ and $\frac{\partial L}{\partial \theta}$ can be computed in a single call to an ODE solver, which concatenates the original state, the adjoint, and the other partial derivatives into a single vector.

Here we assumed that loss function $L$ depends only on the last time point $t_N$. If function $L$ depends also on intermediate time points $t_1, t_2, \dots , t_{N-1}$, we can repeat the adjoint step for each of the intervals $[t_{N-1}, t_N ],[t_{N-2}, t_{N-1}], \dots , [t_0,t_1]$ in the backward order and sum up the obtained gradients.

In practice, most ODE solvers have the option to output the state $z(t)$ at multiple times. When the loss depends on these intermediate states, the reverse-mode derivative must be broken into a sequence of separate solves, one between each consecutive pair of output times. At each observation, the adjoint must be adjusted in the direction of the corresponding partial derivative $\frac{\partial L}{\partial z(t_i)}$.

%TODO ici du code aussi

 


\subsection{Advantages and disadvantages}

In regular neural networks, we consider discrete, individual and independent layers. They are just a block of operations and the neural network consists of those blocks. However, ODE network can be seen as continuous functions. Instead of having separate layers, the entire network is one continuous block of computation. This leads to many advantages but also some disadvantage:
\begin{itemize}
\item The most benefit is that ODENet has more accurate results for time series predictions. Regular neural network have discrete layers, which means they expect the intervals for these time series data sets to be fixed. Therefore, they are bad at predicting output for time series data that is irregular.
\item They have a faster testing time than regular networks, but a slower training time. Thus it's perfect for low power edge computing. There is a trade-off between precision and speed.
\item We can use ordinary differential equations solvers instead of gradient descent. These solvers have a hundred plus years of theory behind them.
\item Lastly, there's a constant memory cost, instead of increasing the cost linearly with each layer in a regular network.
\item Regular neural networks can be evaluated with a fixed amount of computation, and are typically faster to train. In this case, we don't have to choose an error tolerance for a solver.
\end{itemize}




\begin{center}
\includegraphics[scale=0.7]{resnetvsodenet.png}
\end{center}

\textbf{voir def vector field}

\textbf{voir probleme du vanishing gradient dans les nn regulier}

\textbf{ recherche the Universal Approximation Theorem states that, for enough layers or enough parameters, $ML(x)$ can approximate any nonlinear function sufficiently close (subject to some constraints).}


\end{document}